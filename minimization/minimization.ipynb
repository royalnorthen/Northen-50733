{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.6305648963418782, 9)\n",
      "(array([-2.02876577, -4.71238897]), 11)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "z = 1.618 # golden ratio\n",
    "\n",
    "def V_V0(r_sigma): # V/V0 as a function of r/sigma\n",
    "    return (r_sigma)**-6 - np.e**(-r_sigma)\n",
    "\n",
    "def f2(x): # 2d function from nelder mead notebook\n",
    "    return np.sin(x[0]) * np.sin(x[1]) *np.abs(x[0])\n",
    "\n",
    "def gold_rat_sea(func, x1, x4, accuracy=0.001): # this is unfinished and no one needs it\n",
    "    while x4-x1>accuracy:\n",
    "        x2 = x1 + (x4-x1)/z\n",
    "        x3 = x4 - (x4-x1)/z\n",
    "        f2 = func(x2)\n",
    "        f3 = func(x3)\n",
    "        #if f2>f3:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradient_desc(f, x0, step=1, accuracy=0.0001, verbose=False, counting=True):\n",
    "    fprime = (f(x0+1e-8) - f(x0-1e-8)) / 2e-8\n",
    "    step_count = 0\n",
    "    while np.abs(fprime) > accuracy: # both sides were multiplied by step for different reasons\n",
    "        fprime = (f(x0+1e-8) - f(x0-1e-8)) / 2e-8\n",
    "        x0 -= fprime*step\n",
    "        if verbose:\n",
    "            print(x0, fprime, step)\n",
    "        if counting:\n",
    "            step_count +=1\n",
    "    return x0, step_count\n",
    "\n",
    "def partial_deriv_central(f, x0):\n",
    "    dimensions = len(x0)\n",
    "    fpartial = np.zeros(dimensions)\n",
    "\n",
    "    partial_delta = np.zeros(dimensions)\n",
    "    for i in range(dimensions):\n",
    "        partial_delta[i], partial_delta[i-1] = 1e-8, 0\n",
    "        fpartial[i] = (f(x0 + partial_delta) - f(x0 - partial_delta)) / 2e-8\n",
    "\n",
    "    gradient = np.linalg.norm(fpartial) # epic way to get vector magnitude\n",
    "\n",
    "    return fpartial, gradient\n",
    "\n",
    "\n",
    "def Ndim_grad_desc(f, x0, step=0.1, accuracy=0.0001, verbose=False, counting=True, max_steps=10000):\n",
    "    fpartial, gradient = partial_deriv_central(f, x0)\n",
    "    \n",
    "    \n",
    "    step_count = 0\n",
    "    while np.abs(gradient) > accuracy: # both sides were multiplied by step for different reasons\n",
    "        fpartial, gradient = partial_deriv_central(f, x0)\n",
    "        \n",
    "        x0 = x0 - fpartial*step\n",
    "        if verbose:\n",
    "            print(x0, fpartial, step)\n",
    "        if counting:\n",
    "            step_count +=1\n",
    "            if step_count == max_steps+1:\n",
    "                print(f\"Exceeded maximum steps ({max_steps}).  Current output is:\")\n",
    "                break\n",
    "    return x0, step_count\n",
    "\n",
    "print(gradient_desc(V_V0, 1.4))\n",
    "\n",
    "print(Ndim_grad_desc(f2, np.array([-2,-2]), step=0.5))\n",
    "\n",
    "# it's so fast because lightning speed baby\n",
    "# 20 steps for two minimizations, and they're both plenty accurate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
